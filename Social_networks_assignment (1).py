# -*- coding: utf-8 -*-
"""Copy of Social_Networks_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gG6eYVZKn-WGrw6WMGz2cjX6T6AM8XRw

## Social Networks Assignment

We begin with some of the notation introduced in the text.

$$
   \begin{array}[t]{llp{5in}}
   d(i,j)     &  &\text{the length of the shortest path between nodes $i$ and $j$}\\
  \text{N}_{j} &  & \text{the (set of) neighbors of node $j$}\\
   \mid \text{N}_{j} \mid &  & \text{the number of neighbors node $j$ has in the graph}\\
   \text{E}(\text{S}) &  & \text{the set of edges for some set of nodes S}\\
   \text{E}(\text{N}_{j}) & & \text{the set of edges among the neighbors of $j$}\\
   \mid \text{E}(\text{N}_{j})\mid & & \text{the number of edges among the neighbors of $j$}\\
   {k \choose 2} &  \frac{k(k-1)}{2} & \text{the number of possible pairs for a set of $k$ things} \\[.05in]
   {\mid \text{N}_{j} \mid \choose 2} &\frac{\mid \text{N}_{j} \mid(\mid \text{N}_{j} \mid-1)}{2} & \text{the number of possible edges connecting neighbors of $j$} \\[.05in]
   \text{APL}(\text{G}) & \frac{1}{{N \choose 2}} \sum_{i,j} d(i,j) &\text{the average path length in graph G, the sum of the shortest paths between all node
                                                   pairs,}\\
                            & & \text{divided by ${{N \choose 2}}$, the number of node pairs in graph G}
   \end{array}
$$

Note that $i$ is not in the neighborhood set $\text{N}_{i}$.
Then the **clustering coefficient** for a **node** $i$,
written $\text{C}_{i}$, is defined as

$$
\text{C}_{i} = \frac{\mid \text{E}(\text{N}_{i}) \mid}{ {\mid \text{N}_{i} \mid \choose 2}}
$$

As stated above

$${\mid \text{N}_{i} \mid \choose 2}$$

is the maximum possible number of edges connecting neighbors of $i$ (the number
of edges there would be if all of $i$s friends knew each other),
so $\text{C}_{i}$ is the number of **actual** edges between
neighbors of $i$ divided by the maximum **possible** number of
edges between neighbors of $i$.  The clustering
coefficient of a **graph** is just the average of this number for all
nodes in a graph.

As an example,  consider the following graph (similar to the one discussed in the text):
"""

import networkx as nx
G = nx.Graph()


# This is the one we'll start with for this problem set
edges = \
"""
D E
E B
E A
C A
C B
"""
G = nx.parse_edgelist(edges.split('\n'))
nx.draw_networkx(G,node_color='skyblue')

from networkx.algorithms.bipartite.basic import color
import networkx as nx
g = nx.Graph()

a = \
"""
A B
B C
C D
D A
"""

g = nx.parse_edgelist(a.split("\n"))
nx.draw_networkx(g,node_color='blue')

"""**Q1**

Using `G` as draw above, write out the betweenness centrality calculation for node E, using the
definition of betweenness centrality for a node given in the
text chapter.

We reproduce that definition here, for convenience:

We define the **betweenness centrality** of node $j$, $\text{Betweenness}_{j}$ as

$$
   \begin{array}[t]{l}
   \text{Betweenness}_{j} = \sum_{i\neq j\neq k} \frac{\sigma_{ik}(j)}{\sigma_{ik}},
   \end{array}
$$

Here $\sigma_{ik}$ is the total number of shortest paths
from $i$ to $k$; and  $\sigma_{ik}(j)$ is the total
number of such paths which pass through $j$.  The expression
to the right of $\sum$ is proportion of all shortest paths
between $i$ and $k$ that pass through $j$.
The symbol $\sum$ indicates
we sum that quantity for various $i$ and $k$, and
the little subscript tells us $i\neq j\neq k$; that is, we find that quantity
for all distinct nodes $i,k$ in the graph that are different from $j$.

And we add up the results.  Roughly what we're calculating is: What proportion
of the shortest paths in the graph pass through $j$? Or what proportion
of the traffic taking a shortest path journey has to pass through $j$.

To see an example of writing out the calculation (on a different graph!)
look back at the textbook discussion.

**Q2**

How does the betweenness computation for node E in G
change if we add an edge between C and E as shown in the cell below?
"""

edges2 = \
"""
D E
E B
E A
C A
C B
C E
"""
G2 = nx.parse_edgelist(edges2.split('\n'))
nx.draw_networkx(G2,node_color='skyblue')

"""**Q3**

How does the betweenness computation for node E change if we add an edge between A  and B as shown in the cell below?
"""

edges3 = \
"""
D E
E B
E A
C A
C B
A B
"""
G3 = nx.parse_edgelist(edges3.split('\n'))
nx.draw_networkx(G3,node_color='skyblue')

"""**Q4** What is the **degree centrality** of node E in the graph in Question 2?
Show your calculation.  Use `networkx` to check your calculation (the `degree_centrality`
function is demoed un the using_networkx.ipynb notebook.)

**Q5** What is the clustering coefficient for node E in the graph in Question 2?
Show your calculation.
Use `networkx` to check your calculation.


The **clustering coefficient** for a **node** $i$,
written $\text{C}_{i}$, is defined as
$$
\text{C}_{i} = \frac{\mid \text{E}(\text{N}_{i}) \mid}{ {\mid \text{N}_{i} \mid \choose 2}}
$$
The notation $\text{N}_{i}$, read "neighbor set" as explained above, stands for the set of neighbors of $i$
Note that $i$ is not in the neighbor set $\text{N}_{i}$.

As stated above

$${\mid \text{N}_{i} \mid \choose 2}$$

is the maximum possible number of edges connecting neighbors of $i$ (the number
of edges there would be if all of $i$s friends knew each other),
so $\text{C}_{i}$ is the number of **actual** edges between
neighbors of $i$ divided by the maximum **possible** number of
edges between neighbors of $i$.  The clustering
coefficient of a **graph** is just the average of this number for all
nodes in a graph.

**Q7** What is the average shortest path length in the graph in Question 2?
Show your calculations.  Use networkx to check your calculations.
Look [here](https://networkx.org/documentation/networkx-1.3/reference/generated/networkx.average_shortest_path_length.html)
in the `networkx` docs for a discussion of `nx.average_shortest_path_length`.

### More on clustering coefficients

The next cell loads the Florentine Families Graph discussed in the new_using_networkx.ipynb notebook.
This graph is a famous graph of Florentine families (Padgett and Ansell 1993).

>Padgett, John F., and Christopher K. Ansell. "Robust Action and the Rise of the Medici, 1400-1434." American journal of sociology (1993): 1259-1319.

A link represents marriage between members of the families.
For a little background
on what the names and the graph mean, have a quick look at the Padgett and Ansell paper cited above (which is available [here](https://www.jstor.org/stable/pdf/2781822.pdf)), bearing in mind that this partiocular
graph is based on marriage alliances.
"""

from matplotlib import pyplot as plt
plt.figure(figsize=(10,6))

ff = nx.florentine_families_graph()

nx.draw_networkx(ff, with_labels=True, node_shape = 's', node_color="none",
        bbox=dict(facecolor="skyblue", edgecolor='gray', boxstyle='round,pad=0.2'),
                )

""" In the cells below we load two more graphs, Zachary's karate graph and a random graph



Using networkx, compute the clustering coefficent of all three graphs using `networkx`.

>1. the karate graph;
>2. A random graph;  
>3. the Florentone families graph

Try to **guess** the clustering coefficient of the random graph even before you compute it.  Is the clustering coefficient of the karate graph higher or lower than that of the random graph? Is that what you expect given that social networks are small worlds?
"""

G = nx.erdos_renyi_graph(100,0.11)
nx.draw_spring(G,node_size=75)

import networkx as nx

kn = nx.karate_club_graph()
nx.draw_spring(kn,node_size=100)

"""### Centrality

**Q8**

Try using the betweenness centrality and degree centrality measures on the Florentine families graph.  These measures were illustrated in the new_using_networkx.ipynb notebook.  The two measures produce different centrality rankings.   Briefly describe the important differences between the two different centrality rankings
assigned to this graph. Be sure to sort your results for ease of comparison.
"""

for (nm,sc) in sorted(nx.betweenness_centrality(ff).items(),key=lambda x:x[1],reverse=True):
    print(f"{nm:<50} {sc:04.3f}")

for (nm,sc) in sorted(nx.degree_centrality(ff).items(),key=lambda x:x[1],reverse=True):
    print(f"{nm:<50} {sc:04.3f}")

nx.degree_centrality(ff)

"""#### Word Graph Section

We are going to build a graph that depicts letter-sharing relations in a group
of words then answer some questions about it.
"""

import pandas as pd

graph_wds = ['shade', 'pooch', 'fling', 'brief', 'shank', 'juice',
             'boggy', 'sonny', 'parer', 'glogg', 'vague', 'extra',
             'chalk', 'spate', 'soppy', 'graph', 'prick', 'mixer',
             'steal', 'wages','quick']

rows = [[wd, let, i] for wd in graph_wds for (i,let) in enumerate(wd)]
df = pd.DataFrame(rows,columns=["Word","Letter","Position"])

df

df['Word']

"""Each row in `df` represents the occurrence of a letter in a word.
There are 105 rows because each of our 21 words has 5 letters.

We will build  a graph `G` whose nodes are the 21 words in `graph_wds`.  Here is
what the graph represents

>**Two words in `G` will be connected if and only if they share a letter**

`G` could be built without using pandas, but to improve our knowledge
of pandas grouping functionality, we sketch the code that builds the
graph using the `Dataframe` `df` constructed above.  

Your task for **Q1** below will be to write the code that actually builds the
graph.

**Q1 Write code building the letter-sharing graph G from the pandas df above.**

####  Step 1  Grouping the words into letter groups

The first step in building our graph `G` is to group the rows of `df` by the letter column.  This has been done for you in the cell below.
"""

# Create the Series groupby object using df.
# You can convert a groupby instance `gb` into a `SeriesGroupBy` instance by doing `gb[col_name]`
# We do that here so that the members of each group will be the words containing the group letter
let_gps = df.groupby('Letter')['Word']
let_gps

"""#### Getting a letter group

We illustrate what the groupby object looks like/does. In the following cells `cg` is a `SeriesGroupBy` instance created by the code above, where each group in `let_gps` corresponds to a letter (the only letter
not represented in ouyr data is *z*.

For example:
"""

cg1_ser = let_gps.get_group('a')

type(cg1_ser)

"""The `Series` `cg1_ser` contains the words in which `'a'` occurs."""

cg1_ser

len(cg1_ser)

"""So we want our graphg `G` to have one node for
each of the 10 words above and all of them should be
connected, because they all contain the letter `'a'`.
Since there are 45 word pairs: (10*9)/2,
we will need 45 edges  among the 10 words.

The 10-word subgraph of `G` looks like this:
"""

import networkx as nx
(fig,ax) = plt.subplots(1,1, figsize=(4,4))

decad0 = nx.complete_graph(10)
lbls = dict(enumerate(cg1_ser.values))
decad = nx.relabel_nodes(decad0,lbls)
print(f"complete_graph(10) Num nodes: {len(decad)} Num edges: {decad.number_of_edges()}")
nx.draw_networkx(decad,ax=ax,node_color='c',font_size=9,node_size=600)

"""#### How to loop through all the letter groups

The `SeriesGroupBy` instance  `let_gps` is an iterable generating pairs of the form `(let, Ser)`, where  `Ser` is a `pandas Series` containing the group words with `let`. All those words need to be in `G`, and  they all need to be connected in `G`.

We illustrate how to enumerate all the word groups by looping through the first
two groups in `let_gps`:
"""

j = 1
for (i,g) in  enumerate(let_gps):
    # g is a tuple of length 2
    print(len(g))
    # letter from group tuple
    print(g[0])
    # Word group from group tuple: The letter a occurs in 10 words.
    print(type(g[1]),len(g[1]))
    # the entire word group.
    print(g[1])
    # Example stopped after looking at just j+1 groups (letters)
    if i == j:
        break

"""#### Finding all pairs in a sequence

A useful way of looking at it for writing the code is to pair each word with all the words below it in
a word group; using the *a* group above as an example: *shade*
gets paired with 9 other words; *shank* gets paired with 8 others
(it's already paired with *shade*); *parer* gets paired
with 6 others, and so on until *steal* gets paired with *wages*. \
The number of pairs is the number of edges we need to add to the graph
and that number is:
"""

9 + 8 + 7 + 6 + 5 + 4 + 3 + 2 + 1

"""The key computational point is that it works to pair
each word with the words below it on the cast list, and
add an edge for each pair. This works because
when you add the edge

```
(`shade`, `shank`)
```

there is no longer any point in adding the edge

```
(`shank`, `shade`).
```

It's an undirected Graph (no arrows on the edges), so
these are the same edge; `networkx` enforces this,
as is demonstrated in the next cell.
"""

# Making an empty undirected Graph
G = nx.Graph()
print(f"Num edges: {G.number_of_edges()}")
G.add_edge('shade', 'shank')
print(f"Num edges: {G.number_of_edges()}")
## You can retry after commenting out the next line. Both tuples are still in G.
G.add_edge('shank', 'shade')
print(f"Num edges: {G.number_of_edges()}")
print(('shade', 'shank') in G.edges)
print(('shank', 'shade') in G.edges)

"""#### Step 2 Making the word graph

Start with an empty graph:

```
G = nx.Graph()
```

You are going  to build a list of edges (elist) that you ultimately add to
G, using `G.add_edges_from(...)`.

Loop though  `let_gps`:

```
for (let,S1) in  let_gps:
```

On each step of the loop, you find a series `S1` containing the words
that share  the letter `let`.  Each pair of words in `S1` -- for instance, `('vague','wages')` --
is an edge you want to add to G.  So add these pairs to `elist`.

Havong constructed the list of word pairs,
you will find the `G.add_edges_from(...)` method, illustrated below, useful.
Note that `elist` should contain pairs of strings.  Each string should be a word from `graph_words`.
"""

G0 = nx.Graph()
G0.add_edges_from([("aa","bb"),("cc","aa"),("dd","cc"),("bb","cc",),("bb","dd")])
print(f"G: {len(G0)} nodes {len(G0.edges)} edges")
nx.draw_networkx(G0)

"""**Your answer in the next cell:**"""

G = nx.Graph()
for i, row in df.iterrows():
    current_word = row["Word"]
    current_position = row["Position"]
    for j in range(i):
        prev_word = df.loc[j, "Word"]
        prev_position = df.loc[j, "Position"]
        if current_word != prev_word and row["Letter"] in prev_word:
            G.add_edge(current_word, prev_word)
    if current_word.count(row["Letter"]) > 1:
        G.add_edge(current_word, current_word)

"""If you've constructed the graph `G2` correctly, it should have 149 edges and 21 nodes."""

len(G2.edges()),len(G2)

"""If your graph-constructing
code is not working properly, execute the following cell to define `G` so that you can continue
with the rest of the questions:
"""

import networkx as nx
import urllib.request
import os.path

def url_fetch_networkx_graph (url):
    with urllib.request.urlopen(url) as filehandle:
         G = nx.read_gml(filehandle)
    return G

github_networks_data = 'https://raw.githubusercontent.com/gawron/python-for-social-science/master/networks/'
word_graph_url = os.path.join(github_networks_data, 'social_networks_assignment_word_graph.gml')

G2 = url_fetch_networkx_graph (word_graph_url)
G2

len(G2.edges()),len(G2)

"""**Q2** Draw your letter graph with labels to show the words.  To get your labels
to show up nicely you should look at the networkx notebook and modify the
code that draws the Florentine Families Graph with `node_shape='s'` and a
`bbox` argument.
"""

##  Your code goes here.  The graph looks like the one below.

pos = nx.spring_layout(G, seed=42)
plt.figure(figsize=(10,6))
nx.draw_networkx(G, pos, node_size=1000, node_color="none", node_shape='s', bbox=dict(facecolor="skyblue", edgecolor='gray', boxstyle='round,pad=0.2'))
plt.show()



"""**Q3**   Use the graph as a dictionary to retrieve all the nodes connected to `"quick"`.
(NB:  You can do this with `G2.neighbors('quick')` but use `G2` as a dictionary instead).
"""

# Your code goes here.  The output looks like the output below.
G = nx.Graph()
for word in graph_wds:
    G.add_node(word)
for i, row in df.iterrows():
    current_word = row["Word"]
    current_position = row["Position"]
    for j in range(i):
        prev_word = df.loc[j, "Word"]
        prev_position = df.loc[j, "Position"]
        if current_word != prev_word and row["Letter"] in prev_word:
            G.add_edge(current_word, prev_word, weight=1)

neighbors = G.adj['quick']

for neighbor, weight in neighbors.items():
    print(f"{neighbor}: {weight}")

"""**Q3** Find the three words with the highest betweenness centrality in `G`."""

#  Your code will create the betweenness centrality dictionaryu for G abd sort
#  its itemlist

import heapq
bc = nx.betweenness_centrality(G)
top_nodes = heapq.nlargest(3, bc, key=bc.get)

for node in top_nodes:
    print(node, bc[node])

"""**Q3** Find the three words with the highest degree centrality in `G`.

What word has the lowest degree centrality?
"""

# Same as for last problemn but with degree_centrality instead of betweenness centrality.
# And find the word that ranks lowest too.

dc = nx.degree_centrality(G)

top_nodes = heapq.nlargest(3, dc, key=dc.get)

for node in top_nodes:
    print(node, dc[node])

lowest_node = min(dc, key=dc.get)
print("Lowest Value: ",lowest_node, dc[lowest_node])

"""**Q4** What is the average clustering coefficient of `G`?  How does it compare to the
average clustering coefficient of the social network graphs Florentine Families and
Zachary's Karate Club (computed in a previous problem).
"""

##  Your code here
avg = nx.average_clustering(G)
avg

"""### Extra Credit

**Extra Credit One**  Modify your code for creating the word graph in the **Word Graph Section** so that edges are **weighted**.  The weight attribute of an edge should represent the number of letters shared by two words
the edge connects; "wages" and "vague" share three letters, "a", "g" and "e", so the weight of the edge connecting them should be 3. (See the example below).

Hint: Take a look at `elist` and try to understand why there are duplicates.
You may also want to look at `nltk.FreqDist` and/or `nx.set_edge_attributes`.
"""

edges = {}
for i, wd1 in enumerate(graph_wds):
    for j, wd2 in enumerate(graph_wds):
        if i < j:
            overlap = set(wd1).intersection(set(wd2))
            weight = len(overlap)
            if weight > 0:
                edges[(wd1, wd2)] = weight

G2 = nx.Graph()
G2.add_weighted_edges_from([(k[0], k[1], v) for k, v in edges.items()])

"""To illustrate: after you succeed, each edge will have a weight attribute. The attribute dictionary
of an edge can be accessed as follows:
"""

G2['wages']['vague']

"""And of course to look up the weight of an edge:"""

G2['wages']['vague']['weight']

"""This extra credit problem is worth 5 midterm points.  That is, I will add 5 points to your midterm score
if you do this problem correctly.  If you already have a perfect score on the midterm, this extra
credit problem cannot improve your grade. Note:  If you used a downloaded version of the graoh
to do the word graph section, you can get credit for this extra credit problem.

**Extra credit problem 2**.  Redraw your graph to use your edge weights to affect the drawing in one of two ways.  Either have your edge thicknesses determined by your edge weights, or have your edge colors determined by your edge weights. Fir the latter option you will need to use a **color map**.  For true bliss, do both.  This problem is worth another 5 midterm points.   If your midterm score is already equal to 104, doing this problem cannot improve your grade.  Hint:  Read some documentation.  Do some stackoverflow searches.  Review customizing your graph drawing.   Note:  If you used a downloaded version of the graoh
to do the word graph section, you can still get credit for this extra credit problem because the
downloaded version of the graph (called `G2` above) does have edge weights.
"""

import networkx as nx
import matplotlib.pyplot as plt

G3 = nx.Graph()
for word in graph_wds:
    G3.add_node(word)
for i in range(len(graph_wds)):
    for j in range(i+1, len(graph_wds)):
        w1, w2 = graph_wds[i], graph_wds[j]
        weight = len(set(w1) & set(w2))
        if weight > 0:
            G3.add_edge(w1, w2, weight=weight)

pos = nx.spring_layout(G3)
edge_weights = nx.get_edge_attributes(G3, 'weight')
cmap = plt.cm.Blues
nx.draw_networkx_nodes(G3, pos, node_size=500)
nx.draw_networkx_labels(G3, pos, font_size=10)
nx.draw_networkx_edges(G3, pos, edge_cmap=cmap, edge_vmin=0, edge_vmax=max(edge_weights.values()), edge_color=list(edge_weights.values()))
nx.draw_networkx_edge_labels(G3, pos, edge_labels=edge_weights, font_size=8)
plt.axis('off')
plt.show()

"""**Extra credit problem 3:**  Modify your graph creation code so that  the weight  of the edge connecting
two words is not affected by duplicate letters in either of the words.  Consider the following
edge weight:
"""

def count_unique_letters(w1, w2):
    return (len(w1) + len(w2)) - len(set(w1)^set(w2))

rows = []
for i, wd1 in enumerate(graph_wds):
    for wd2 in graph_wds[i+1:]:
        count = count_unique_letters(wd1, wd2)
        if count > 0:
            rows.append((wd1, wd2, count))

df = pd.DataFrame(rows, columns=['Word1', 'Word2', 'Weight'])
G2 = nx.from_pandas_edgelist(df, 'Word1', 'Word2', 'Weight')

G2["glogg"]['boggy']

"""This is the answer returned by the simplest approach to computing edge weights (based
on the edge counts in `elist`).  Modify your code so that duplicate letters are ignored in
computing edge weights and the weight of the edge above (if you called your graph `G3`) is:

```
>>> G3["glogg"]['boggy']
{'weight': 2}
```

Since the only shared latters, not counting duplicates, are "g" and "o". Note that
all the weights involving words with no duplicate letters
should be unchaged:

```
>>> G3['wages']['vague']['weight']
3
```

If you have already
done this, you can get credit for this problem by explaining how your code avoids
overweight edges due to duplicate letters. This problem is worth another 5 midterm points.   If your midterm score is already equal to 104, doing this problem cannot improve your grade. Note:  If you used a downloaded version of the graoh to do the word graph section, you can still get credit for this extra credit problem by modifying the edge
weights in `G2`, which you downloaded.  Use
`nx.get_edge_attributes` and `nx.set_edge_attributes`.  You will have to do a computation that computes the
correct edge weight for each word pair and then set  the edge attributes of `G2` to the corrected version.
"""

