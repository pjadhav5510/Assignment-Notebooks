# -*- coding: utf-8 -*-
"""Copy of Insults_with_Naive_Bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1US0Gi2Lzdu4JFUg4-DrdU9O13jw7wqI4

## Classifying text
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV as gs
import sklearn.feature_extraction.text as text
import sklearn.naive_bayes as nb
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, accuracy_score
# %matplotlib inline

"""We turn to applying machine learning classification methods to text. There are
no new principles at stake.  In principle, everything is the same as it was for
learning how to classify irises.

1.  We need to find labeled data; each of the exemplars in the data should be represented with a fixed set of features.  
2. We need to split our data and training and test data.  
3. We need to train learner on the training data and evaluate it (test it) it on the test data.

The problem is that text data is not in a form  that is compatible with
what we have learned about classifiers.  The text must be put in a suitable
form before a linear model; can be trained on it.

**Training**

1.  Labeled data must be loaded (into Python).  It should be a sequence of documents T accompanied by a sequence of labels L.
2.  Split T and L into training and test groups, yielding T1 and T2; as well as and L1 and L2.
2.  Train or a **feature model** on the training data T1 (or in scikit learn terminology **fit** the model **to** the training data).  The feature model inputs the text sequence and outputs a **term-document** matrix suitable for training a linear classifier.  The feature model is called a **vectorizer**
(because it turns a document into a vector, a column of numbers).
3.  Using the trained vectorizer, transform T1 into a term document matrix M1.
4.  Train a linear model $\mu$ on M1 and L1.

**Evaluation**

1.  Transform the test data T2 into a term document matrix M2 using the vectorizer fit during step 2 of training;  in particular this means if there are words in the T2 data that were never seen during training, they are ignored in building M2.
2.  Use $\mu$  to classify the texts represented in M2; that is produce a set of predicted labels P2.
3.  Compare the actual labels L2 with the predicted labels P2 using standard evaluation metrics such as precision, accuracy, and recall.

## Review the steps with insult detection

We looked at the insult detection data in  the text classification notebook.

### Training step 1: Loading the data

Let's load the CSV file.
"""

import os.path
site = 'https://raw.githubusercontent.com/gawron/python-for-social-science/master/'\
'text_classification/'
#site = 'https://gawron.sdsu.edu/python_for_ss/course_core/book_draft/_static/'
df = pd.read_csv(os.path.join(site,"troll.csv"))

"""Each row is a comment  taken from a blog or online forum. There are three columns: whether the comment is insulting (1) or not (0), the date, and the comment."""

df.tail()

"""Now we define the text sequences $\mathbf{T}$ and the label sequence  $\mathbf{L}$."""

T = df['Comment']

L = df['Insult']

"""### Step 2 Split the data and labels into training and test groups"""

T1, T2, L1, L2 = train_test_split(T,L)

"""### Step 3 and 4:  Fit the feature model (vectorizer) to the training data and Transform  it"""

tf = text.TfidfVectorizer()
# Scikit learn has one function that does both fitting and transforming.
# M1 is the transformed data
# tf is the trained feature model (which will be used to transform the test data)
M1 = tf.fit_transform(T1)

"""### Step 5 Training the classifier

Now, we are going to train a classifier as usual. We first split the data into a train and test set.

We use a **Bernoulli Naive Bayes classifier**.
"""

# Create classifer
bnb =nb.BernoulliNB()

# Fit (train) the classifier  using the training data and labels
bnb.fit(M1, L1);

"""### Evaluation

Evaluate the classifier, first using accuracy (what `.score()` returns).
"""

# vectorize the test data using the vectorizer trained on T1
# Notice we DONT call .fit_transform() because that would retrain the vectorizer on the test data
# We call .transform() using the trained model to transform the new data.
# Words not seen during training will be ignored.
M2 = tf.transform(T2)
# Classify the data using the trained classisifer and report the accuracy
bnb.score(M2, L2)

"""Now try re-executing steps 2 through 5.  (Just re-execute the cells)  The results should be the same, right?

Well, are they?  

What happens:  each training test split produces a different set of test data.  Sometimes the test is harder.
Sometimes it's easier.  Or looking at it another way:  Sometimes the training data is a better preparation for the test than others.  

To get a realistic view of how our classifier is doing we take the average performance on a  number of
train/test splits.  This is called **cross validation**.  We return to that point below.

#### Using all three evaluation metrics

First let's get more evaluation numbers, in particular precision and recall.  We do
that by calling a method that returns the predicted labels P2, so we can compare
L2 and P2 using different evaluation metrics.
"""

P2 = bnb.predict(M2)
scores = np.array([accuracy_score(P2, L2),
                   precision_score(P2, L2),
                   recall_score(P2, L2)])
print(f'Accuracy: {scores[0]:.2f} Precision: {scores[1]:.2f} Recall: {scores[2]:.2f}')

"""We see that the accuracy is a bit misleading.  There is a serious precision problem.

What does that mean in the setting of insult detection?  It means the BNB classifier is a little too
eager to call something an insult.  When it flags something as an insult, it
is right only 14% of the time.

Why would that be?  Think about how the model is trained and what its weakness might be.
This is what it means to try to interpret or discuss a model's performance.  Zoom
in the model's weakness. Talk about where that weakness comes from.

#### Basic train and test loop

How to get the average of a number of runs.
"""

def split_fit_and_eval(T,L,test_size=.2):
    # This code just collects together the training steps 2-5 + the eval
    # That is, It does one training,test., eval run
    (T1, T2, L1, L2) = train_test_split(T, L, test_size=test_size)
    tf = text.TfidfVectorizer()
    M1 = tf.fit_transform(T1)
    bnb = nb.BernoulliNB()
    bnb.fit(M1,L1)
    # .fit(), ..fit_transform()
    M2 = tf.transform(T2)
    P2 = bnb.predict(M2)
    return np.array([accuracy_score(P2,L2),
                     precision_score(P2, L2),
                     recall_score(P2,L2)])

# Split, Train, test and eval 10 times
num_runs = 10
# an accumulator for acc.,pre.,and rec.
scores = np.zeros((3,))
for test_run in range(num_runs):
    scores += split_fit_and_eval(T,L)
# Compute the average of the num_runs runs for all metrics
normed_stats = scores/num_runs

print(f'Accuracy: {normed_stats[0]:.2f} Precision: {normed_stats[1]:.2f} Recall: {normed_stats[2]:.2f}')

"""## Homework

Read the on line book draft chapter about text classification and and especially
about  movie review data.  Note that you will be using a different classifier implementation (`scikit_learn`) than the one used in the book
(`nltk`).  Therefore, when it comes to writing code for training the calssifier. focus on the code examples in this notebook, which use `scikit_learn`.

Try using two classifiers on the movie review data, the one used in the textbook, an SVM, and
the Bernoulli Naive Bayes model used above. Be sure
to stick with  scikit learn (it has an SVM implementation).
Some points of emphasis;

1.  Be sure to get the average of at runs  least 10 runs for **both** classifiers.
2.  Be sure to get average accuracy, precision, and recall for both classifiers on those multiple runs. You will probably find `split_fit_and_eval` defined above useful, but you may need to modify it.
3.  For your first discussion post turn in the new code you wrote, including the code that labels and shuffles the data (discussed further below).  If you have to do a new import, show that. If you have to rewrite `split_fit_and_eval`, turn in the new version.  Also show the output, which should be a single line giving the accuracy, prcision, and recall.
4.  Discuss which classifier does better.  Discuss which metric the best classifier does the worst at and speculate as to why (this will require reviewing the definitions of precision and recall and thinking about what they mean in a movie review setting).

#### Help with getting the movie reviews data.

Execute the next two cells to get the movie review data.
"""

import nltk
nltk.download('movie_reviews')

from nltk.corpus import movie_reviews as mr

def get_file_strings (corpus, file_ids):
    return [corpus.raw(file_id) for file_id in file_ids]

data = dict(pos = mr.fileids('pos'),
            neg = mr.fileids('neg'))

pos_file_ids = data['pos']
neg_file_ids = data['neg']

# Get all the positive and negative reviews.
pos_file_reviews = get_file_strings (mr, pos_file_ids)
neg_file_reviews = get_file_strings (mr, neg_file_ids)

"""Each review is a string.  In principle, a list of strings like `pos_file_reviews`  can be passed to `text.TfidfVectorizer()` via the `fit_transform` method to train a vectorizer for machine learning.
You could code that up.

What you'd really like to do is use `split_fit_and_eval`, defined above, which does a lot of the work for you.

But hold on. You have a coding problem. You don't have  a sequence of documents and labels.  Instead you have
one sequence of positive documents  and another sequence of negative documents.  

So you will need to turn those two sequences into a sequence of documents and a sequence of labels
because that's what `split_fit_and_eval` wants.  You also want the doc sequence
to contain a random mixture of positive and negative documents, because some machine
learning algorithms are sensitive to the order in which training data is presented to
them.

The next cell does **not** do that for you.  But it illustrates an approach using
two sets of English letters in place of two sets of English documents.
"""

# Lets work on letters instead of documents
# There are 2 classes, letters from the first half of the
# alphabet ('f') and letters frmm the last half ('l')

from random import shuffle
from string import ascii_lowercase

#Class 1 of the letters: the f_lets
f_lets = ascii_lowercase[:13]
print(f_lets)
#Class2 of the letters: the l_lets
l_lets = ascii_lowercase[13:]
print(l_lets)

# Now get pairs of letters and labels
f_pairs = [(let,'f') for let in f_lets]
l_pairs = [(let,'l') for let in l_lets]

###########  Shuffling  ###########################
# Way too orderly, the classes arent mixed yet.
data = f_pairs + l_pairs
shuffle(data)
###################  Now they're shuffled! ###############

# Separate the letters from their labels
lets, lbls = zip(*data)
print(lets)
print(lbls)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
import numpy as np

# Load movie review data
from nltk.corpus import movie_reviews
negids = movie_reviews.fileids('neg')
posids = movie_reviews.fileids('pos')
negdocs = [movie_reviews.raw(fileid) for fileid in negids]
posdocs = [movie_reviews.raw(fileid) for fileid in posids]
docs = negdocs + posdocs
labels = [0]*len(negdocs) + [1]*len(posdocs)

# Preprocess data using bag-of-words approach
vectorizer = CountVectorizer(max_features=2000)
X = vectorizer.fit_transform(docs)
y = np.array(labels)

# Define function for splitting data and evaluating classifier
def split_fit_and_eval(clf):
    accs, precisions, recalls = [], [], []
    for i in range(10):
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        report = classification_report(y_test, y_pred, output_dict=True)
        accs.append(report['accuracy'])
        precisions.append(report['macro avg']['precision'])
        recalls.append(report['macro avg']['recall'])
    avg_acc = np.mean(accs)
    avg_prec = np.mean(precisions)
    avg_recall = np.mean(recalls)
    return avg_acc, avg_prec, avg_recall

# Train and evaluate SVM classifier
svm_clf = SVC(kernel='linear')
svm_avg_acc, svm_avg_prec, svm_avg_recall = split_fit_and_eval(svm_clf)
print("SVM Classifier:")
print("Average Accuracy:", svm_avg_acc)
print("Average Precision:", svm_avg_prec)
print("Average Recall:", svm_avg_recall)

# Train and evaluate Bernoulli Naive Bayes classifier
bnb_clf = BernoulliNB()
bnb_avg_acc, bnb_avg_prec, bnb_avg_recall = split_fit_and_eval(bnb_clf)
print("Bernoulli Naive Bayes Classifier:")
print("Average Accuracy:", bnb_avg_acc)
print("Average Precision:", bnb_avg_prec)
print("Average Recall:", bnb_avg_recall)